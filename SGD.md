**SGD(Stochastic Gradient Descent)：随机梯度下降**，通过每个样本迭代更新一次，在随机梯度下降中，随机选择几个样本而不是每次迭代整个数据集。在BGD中，batch被视为整个数据集。在SGD中，执行每次迭代计算梯度仅使用单个样本，即batch大小为1。被选择用于迭代的样本被随机打乱

常见的梯度下降有三种形式：BGD、SGD、MBGD，它们的不同之处在于我们使用多少数据来计算目标函数的梯度，

梯度是函数的斜率。它衡量一个变量响应另一个变量的变化而变化的程度，梯度越大，坡度越陡。从初始值开始，迭代运行梯度下降以找到参数的最佳值，以找到给定成本函数的最小可能值。梯度下降是一种优化算法，通常用于寻找深度学习算法中的权值及系数，如逻辑回归。它的工作原理是让模型对训练数据进行预测，并使用预测中的error来更新模型从而减少error，找到使模型在训练数据集上的误差最小化的模型参数

SGD算法中的一个关键参数是**学习率**。在实践中，有必要随着时间的推移逐渐降低学习率。SGD中梯度估计引入的噪声源(m个训练样本的随机采样)并不会在极小点处消失。相比之下，当我们使用批量梯度下降到达极小点时，整个代价函数的真实梯度会变得很小，之后为0，因此批量梯度下降可以使用固定的学习率

<img src="C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20250925221656233.png" alt="image-20250925221656233" style="zoom:50%;" />

![image-20250925221820887](assets/image-20250925221820887.png)